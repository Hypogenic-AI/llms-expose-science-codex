Reading prompt from stdin...
{"type":"thread.started","thread_id":"019b1f93-3757-7de3-ac9f-8aabbdbd88ca"}
{"type":"turn.started"}
{"type":"item.completed","item":{"id":"item_0","type":"reasoning","text":"**Starting with directory check**"}}
{"type":"item.started","item":{"id":"item_1","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_1","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/llms-expose-science-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_2","type":"reasoning","text":"**Planning approach for large task**"}}
{"type":"item.started","item":{"id":"item_3","type":"todo_list","items":[{"text":"Collect keywords and shortlist target papers (search APIs/web) and download PDFs to papers/","completed":false},{"text":"Identify datasets, download/snapshot small samples, and document dataset instructions in datasets/","completed":false},{"text":"Find and clone relevant code repos to code/ with documentation","completed":false},{"text":"Write literature_review.md and resources.md; create papers/datasets/code READMEs; finalize completion marker","completed":false}]}}
{"type":"item.completed","item":{"id":"item_4","type":"reasoning","text":"**Checking directory existence**"}}
{"type":"item.started","item":{"id":"item_5","type":"command_execution","command":"/bin/bash -lc ls","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_5","type":"command_execution","command":"/bin/bash -lc ls","aggregated_output":"artifacts\nlogs\nresults\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_6","type":"reasoning","text":"**Preparing directories and keywords**\n\nPlanning to create directories for papers, datasets, and code after verifying the current working directory; also considering gathering keywords via online search, possibly using arXiv."}}
{"type":"item.started","item":{"id":"item_7","type":"command_execution","command":"/bin/bash -lc 'mkdir -p papers datasets code'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_7","type":"command_execution","command":"/bin/bash -lc 'mkdir -p papers datasets code'","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_8","type":"reasoning","text":"**Confirming directory before .gitignore creation**\n\nI'll run pwd to verify the current path before creating the .gitignore and README to ensure the files go in the right place."}}
{"type":"item.started","item":{"id":"item_9","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_9","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/llms-expose-science-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_10","type":"reasoning","text":"**Adding .gitignore to dataset directory**"}}
{"type":"item.started","item":{"id":"item_11","type":"command_execution","command":"/bin/bash -lc \"cat > datasets/.gitignore << 'EOF'\n# Exclude dataset payloads\n*\n\n# Keep docs and lightweight samples\n\"'!.gitignore\n!README.md\n!**/README.md\n!**/samples/\n!**/download.sh\n!**/download_instructions.txt\nEOF'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_11","type":"command_execution","command":"/bin/bash -lc \"cat > datasets/.gitignore << 'EOF'\n# Exclude dataset payloads\n*\n\n# Keep docs and lightweight samples\n\"'!.gitignore\n!README.md\n!**/README.md\n!**/samples/\n!**/download.sh\n!**/download_instructions.txt\nEOF'","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_12","type":"reasoning","text":"**Planning targeted paper search**\n\nI'm identifying key keywords and candidate papers around LLM-driven scientific discovery and literature gap detection, aiming to quickly find and download relevant recent arXiv PDFs using API queries or curl. Next, I'll execute targeted searches to confirm and retrieve top papers for review."}}
{"type":"item.started","item":{"id":"item_13","type":"command_execution","command":"/bin/bash -lc \"curl -s 'http://export.arxiv.org/api/query?search_query=all:AI%20scientist&start=0&max_results=5' | head\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_13","type":"command_execution","command":"/bin/bash -lc \"curl -s 'http://export.arxiv.org/api/query?search_query=all:AI%20scientist&start=0&max_results=5' | head\"","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_14","type":"reasoning","text":"**Testing curl output handling**\n\nI'm suspecting the curl output is blank or truncated with head, so I'll try capturing the first lines using sed to check if data is actually coming through."}}
{"type":"item.started","item":{"id":"item_15","type":"command_execution","command":"/bin/bash -lc \"curl -s 'http://export.arxiv.org/api/query?search_query=all:AI%20scientist&start=0&max_results=5' | sed -n '1,20p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_15","type":"command_execution","command":"/bin/bash -lc \"curl -s 'http://export.arxiv.org/api/query?search_query=all:AI%20scientist&start=0&max_results=5' | sed -n '1,20p'\"","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_16","type":"reasoning","text":"**Suspecting network block causing blank response**"}}
{"type":"item.started","item":{"id":"item_17","type":"command_execution","command":"/bin/bash -lc \"curl -s 'https://export.arxiv.org/api/query?search_query=all:scientific%20discovery%20language%20models&start=0&max_results=2' | sed -n '1,20p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_17","type":"command_execution","command":"/bin/bash -lc \"curl -s 'https://export.arxiv.org/api/query?search_query=all:scientific%20discovery%20language%20models&start=0&max_results=2' | sed -n '1,20p'\"","aggregated_output":"<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/P5c50nezOuGH9+QwlUp1+KdIvQs</id>\n  <title>arXiv Query: search_query=all:scientific OR all:discovery OR all:language OR all:models&amp;id_list=&amp;start=0&amp;max_results=2</title>\n  <updated>2025-12-15T01:15:34Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=all:scientific+OR+(all:discovery+OR+(all:language+OR+all:models))&amp;start=0&amp;max_results=2&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>2</opensearch:itemsPerPage>\n  <opensearch:totalResults>1175768</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/2406.10833v3</id>\n    <title>A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery</title>\n    <updated>2024-09-28T23:58:10Z</updated>\n    <link href=\"https://arxiv.org/abs/2406.10833v3\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2406.10833v3\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>In many scientific fields, large language models (LLMs) have revolutionized the way text and other modalities of data (e.g., molecules and proteins) are handled, achieving superior performance in various applications and augmenting the scientific discovery process. Nevertheless, previous surveys on scientific LLMs often concentrate on one or two fields or a single modality. In this paper, we aim to provide a more holistic view of the research landscape by unveiling cross-field and cross-modal connections between scientific LLMs regarding their architectures and pre-training techniques. To this end, we comprehensively survey over 260 scientific LLMs, discuss their commonalities and differences, as well as summarize pre-training datasets and evaluation tasks for each field and modality. Moreover, we investigate how LLMs have been deployed to benefit scientific discovery. Resources related to this survey are available at https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2024-06-16T08:03:24Z</published>\n    <arxiv:comment>35 pages; Accepted to EMNLP 2024 (Project Page: https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models)</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CL\"/>\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_18","type":"reasoning","text":"**Planning topic queries**\n\nI'm preparing to gather 5–10 papers on themes like scientific discovery LLMs, automated science agents, and literature gap detection, aiming to select about six relevant works for extraction. Next, I'll form targeted arXiv search queries for these topics."}}
{"type":"item.started","item":{"id":"item_19","type":"command_execution","command":"/bin/bash -lc \"curl -s 'https://export.arxiv.org/api/query?search_query=all:AI%20Scientist&start=0&max_results=3' | sed -n '1,80p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_19","type":"command_execution","command":"/bin/bash -lc \"curl -s 'https://export.arxiv.org/api/query?search_query=all:AI%20Scientist&start=0&max_results=3' | sed -n '1,80p'\"","aggregated_output":"<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/U4AsjvOIi15n2IoMVz8FB64g/o0</id>\n  <title>arXiv Query: search_query=all:AI OR all:Scientist&amp;id_list=&amp;start=0&amp;max_results=3</title>\n  <updated>2025-12-15T01:15:38Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=all:AI+OR+all:Scientist&amp;start=0&amp;max_results=3&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>3</opensearch:itemsPerPage>\n  <opensearch:totalResults>53292</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/2501.02842v1</id>\n    <title>Foundations of GenIR</title>\n    <updated>2025-01-06T08:38:29Z</updated>\n    <link href=\"https://arxiv.org/abs/2501.02842v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2501.02842v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>The chapter discusses the foundational impact of modern generative AI models on information access (IA) systems. In contrast to traditional AI, the large-scale training and superior data modeling of generative AI models enable them to produce high-quality, human-like responses, which brings brand new opportunities for the development of IA paradigms. In this chapter, we identify and introduce two of them in details, i.e., information generation and information synthesis. Information generation allows AI to create tailored content addressing user needs directly, enhancing user experience with immediate, relevant outputs. Information synthesis leverages the ability of generative AI to integrate and reorganize existing information, providing grounded responses and mitigating issues like model hallucination, which is particularly valuable in scenarios requiring precision and external knowledge. This chapter delves into the foundational aspects of generative models, including architecture, scaling, and training, and discusses their applications in multi-modal scenarios. Additionally, it examines the retrieval-augmented generation paradigm and other methods for corpus modeling and understanding, demonstrating how generative AI can enhance information access systems. It also summarizes potential challenges and fruitful directions for future studies.</summary>\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-01-06T08:38:29Z</published>\n    <arxiv:comment>Chapter 2 of the book on Information Access in the Era of Generative AI</arxiv:comment>\n    <arxiv:primary_category term=\"cs.IR\"/>\n    <author>\n      <name>Qingyao Ai</name>\n    </author>\n    <author>\n      <name>Jingtao Zhan</name>\n    </author>\n    <author>\n      <name>Yiqun Liu</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.04583v2</id>\n    <title>Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper</title>\n    <updated>2025-11-10T15:05:28Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.04583v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.04583v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, and iteratively conducts experiments until improvements are realized, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. Through our experiments, the Jr. AI Scientist successfully generated new research papers that build upon real NeurIPS, IJCV, and ICLR works by proposing and implementing novel methods. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We believe this study clarifies the current role and limitations of AI Scientist systems, offering insights into the areas that still require human expertise and the risks that may emerge as these systems evolve.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-06T17:37:49Z</published>\n    <arxiv:comment>Issues, comments, and questions are all welcome in https://github.com/Agent4Science-UTokyo/Jr.AI-Scientist</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Atsuyuki Miyai</name>\n    </author>\n    <author>\n      <name>Mashiro Toyooka</name>\n    </author>\n    <author>\n      <name>Takashi Otonari</name>\n    </author>\n    <author>\n      <name>Zaiying Zhao</name>\n    </author>\n    <author>\n      <name>Kiyoharu Aizawa</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2509.00961v1</id>\n    <title>Ultra Strong Machine Learning: Teaching Humans Active Learning Strategies via Automated AI Explanations</title>\n    <updated>2025-08-31T19:04:31Z</updated>\n    <link href=\"https://arxiv.org/abs/2509.00961v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2509.00961v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Ultra Strong Machine Learning (USML) refers to symbolic learning systems that not only improve their own performance but can also teach their acquired knowledge to quantifiably improve human performance. In this work, we present LENS (Logic Programming Explanation via Neural Summarisation), a neuro-symbolic method that combines symbolic program synthesis with large language models (LLMs) to automate the explanation of machine-learned logic programs in natural language. LENS addresses a key limitation of prior USML approaches by replacing hand-crafted explanation templates with scalable automated generation. Through systematic evaluation using multiple LLM judges and human validation, we demonstrate that LENS generates superior explanations compared to direct LLM prompting and hand-crafted templates. To investigate whether LENS can teach transferable active learning strategies, we carried out a human learning experiment across three related domains. Our results show no significant human performance improvements, suggesting that comprehensive LLM responses may overwhelm users for simpler problems rather than providing learning support. Our work provides a solid foundation for building effective USML systems to support human learning. The source code is available on: https://github.com/lun-ai/LENS.git.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-08-31T19:04:31Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Lun Ai</name>\n    </author>\n    <author>\n      <name>Johannes Langer</name>\n    </author>\n    <author>\n      <name>Ute Schmid</name>\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_20","type":"reasoning","text":"**Refining paper search strategy**\n\nI'm narrowing focus to accessible and relevant recent papers, aiming to identify key works on scientific language models and literature gap detection. Next, I'll target more precise queries like \"literature gap detection language model\" to find five fitting papers."}}
{"type":"item.started","item":{"id":"item_21","type":"command_execution","command":"/bin/bash -lc \"curl -s 'https://export.arxiv.org/api/query?search_query=all:literature%20review%20large%20language%20model&start=0&max_results=3' | sed -n '1,80p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_21","type":"command_execution","command":"/bin/bash -lc \"curl -s 'https://export.arxiv.org/api/query?search_query=all:literature%20review%20large%20language%20model&start=0&max_results=3' | sed -n '1,80p'\"","aggregated_output":"<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/x75r7BK1Mx6HCZOqchJ+b7fi1YI</id>\n  <title>arXiv Query: search_query=all:literature OR all:review OR all:large OR all:language OR all:model&amp;id_list=&amp;start=0&amp;max_results=3</title>\n  <updated>2025-12-15T01:15:44Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=all:literature+OR+(all:review+OR+(all:large+OR+(all:language+OR+all:model)))&amp;start=0&amp;max_results=3&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>3</opensearch:itemsPerPage>\n  <opensearch:totalResults>1466294</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/2411.18583v1</id>\n    <title>Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation</title>\n    <updated>2024-11-27T18:27:07Z</updated>\n    <link href=\"https://arxiv.org/abs/2411.18583v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2411.18583v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>This research presents and compares multiple approaches to automate the generation of literature reviews using several Natural Language Processing (NLP) techniques and retrieval-augmented generation (RAG) with a Large Language Model (LLM). The ever-increasing number of research articles provides a huge challenge for manual literature review. It has resulted in an increased demand for automation. Developing a system capable of automatically generating the literature reviews from only the PDF files as input is the primary objective of this research work. The effectiveness of several Natural Language Processing (NLP) strategies, such as the frequency-based method (spaCy), the transformer model (Simple T5), and retrieval-augmented generation (RAG) with Large Language Model (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR dataset is chosen for this research experiment and three distinct techniques are utilized to implement three different systems for auto-generating the literature reviews. The ROUGE scores are used for the evaluation of all three systems. Based on the evaluation, the Large Language Model GPT-3.5-turbo achieved the highest ROUGE-1 score, 0.364. The transformer model comes in second place and spaCy is at the last position. Finally, a graphical user interface is created for the best system based on the large language model.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2024-11-27T18:27:07Z</published>\n    <arxiv:comment>Key Words : T5, SpaCy, Large Language Model, GPT, ROUGE, Literature Review, Natural Language Processing, Retrieval-augmented generation</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Nurshat Fateh Ali</name>\n    </author>\n    <author>\n      <name>Md. Mahdi Mohtasim</name>\n    </author>\n    <author>\n      <name>Shakil Mosharrof</name>\n    </author>\n    <author>\n      <name>T. Gopi Krishna</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2308.02443v1</id>\n    <title>AI Literature Review Suite</title>\n    <updated>2023-07-27T17:30:31Z</updated>\n    <link href=\"https://arxiv.org/abs/2308.02443v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2308.02443v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>The process of conducting literature reviews is often time-consuming and labor-intensive. To streamline this process, I present an AI Literature Review Suite that integrates several functionalities to provide a comprehensive literature review. This tool leverages the power of open access science, large language models (LLMs) and natural language processing to enable the searching, downloading, and organizing of PDF files, as well as extracting content from articles. Semantic search queries are used for data retrieval, while text embeddings and summarization using LLMs present succinct literature reviews. Interaction with PDFs is enhanced through a user-friendly graphical user interface (GUI). The suite also features integrated programs for bibliographic organization, interaction and query, and literature review summaries. This tool presents a robust solution to automate and optimize the process of literature review in academic and industrial research.</summary>\n    <category term=\"cs.DL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.HC\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2023-07-27T17:30:31Z</published>\n    <arxiv:comment>7 Pages, 5 figures, Keywords: Literature Review, Artificial Intelligence, Text Embeddings, Large Language Models</arxiv:comment>\n    <arxiv:primary_category term=\"cs.DL\"/>\n    <author>\n      <name>David A. Tovar</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2402.11651v2</id>\n    <title>Learning From Failure: Integrating Negative Examples when Fine-tuning Large Language Models as Agents</title>\n    <updated>2024-04-16T11:41:13Z</updated>\n    <link href=\"https://arxiv.org/abs/2402.11651v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2402.11651v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large language models (LLMs) have achieved success in acting as agents, which interact with environments through tools such as search engines. However, LLMs are optimized for language generation instead of tool use during training or alignment, limiting their effectiveness as agents. To resolve this problem, previous work has first collected interaction trajectories between LLMs and environments, using only trajectories that successfully finished the task to fine-tune smaller models, making fine-tuning data scarce and acquiring it both difficult and costly. Discarding failed trajectories also leads to significant wastage of data and resources and limits the possible optimization paths during fine-tuning. In this paper, we argue that unsuccessful trajectories offer valuable insights, and LLMs can learn from these trajectories through appropriate quality control and fine-tuning strategies. By simply adding a prefix or suffix that tells the model whether to generate a successful trajectory during training, we improve model performance by a large margin on mathematical reasoning, multi-hop question answering, and strategic question answering tasks. We further analyze the inference results and find that our method provides a better trade-off between valuable information and errors in unsuccessful trajectories. To our knowledge, we are the first to demonstrate the value of negative trajectories and their application in agent-tunning scenarios. Our findings offer guidance for developing better agent-tuning methods and low-resource data usage techniques.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2024-02-18T17:10:07Z</published>\n    <arxiv:comment>Agent, LLM, Large Language Model</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Renxi Wang</name>\n    </author>\n    <author>\n      <name>Haonan Li</name>\n    </author>\n    <author>\n      <name>Xudong Han</name>\n    </author>\n    <author>\n      <name>Yixuan Zhang</name>\n    </author>\n    <author>\n      <name>Timothy Baldwin</name>\n    </author>\n  </entry>\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_22","type":"reasoning","text":"**Planning to search research gap detection**\n\nConsidering exploring \"Science of science\" and recent work like a 2024 paper on LLM-enhanced research gap detection to find methods for identifying neglected topics. Next, I'll search specifically for \"research gap detection\" literature."}}
{"type":"item.started","item":{"id":"item_23","type":"command_execution","command":"/bin/bash -lc \"curl -s 'https://export.arxiv.org/api/query?search_query=all:research%20gap%20detection&start=0&max_results=3' | sed -n '1,80p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_23","type":"command_execution","command":"/bin/bash -lc \"curl -s 'https://export.arxiv.org/api/query?search_query=all:research%20gap%20detection&start=0&max_results=3' | sed -n '1,80p'\"","aggregated_output":"<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/OACjMe3WU+zoehsoA4YcPkC42gM</id>\n  <title>arXiv Query: search_query=all:research OR all:gap OR all:detection&amp;id_list=&amp;start=0&amp;max_results=3</title>\n  <updated>2025-12-15T01:15:48Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=all:research+OR+(all:gap+OR+all:detection)&amp;start=0&amp;max_results=3&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>3</opensearch:itemsPerPage>\n  <opensearch:totalResults>497603</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/1001.2665v1</id>\n    <title>Detecting Botnets Through Log Correlation</title>\n    <updated>2010-01-15T11:35:17Z</updated>\n    <link href=\"https://arxiv.org/abs/1001.2665v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/1001.2665v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>  Botnets, which consist of thousands of compromised machines, can cause significant threats to other systems by launching Distributed Denial of Service (SSoS) attacks, keylogging, and backdoors. In response to these threats, new effective techniques are needed to detect the presence of botnets. In this paper, we have used an interception technique to monitor Windows Application Programming Interface (API) functions calls made by communication applications and store these calls with their arguments in log files. Our algorithm detects botnets based on monitoring abnormal activity by correlating the changes in log file sizes from different hosts.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2010-01-15T11:35:17Z</published>\n    <arxiv:comment>4 pages, 7 figures, Workshop on Monitoring, Attack Detection and Mitigation (MonAM2006)</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <arxiv:journal_ref>Proceedings of the Workshop on Monitoring, Attack Detection and Mitigation (MonAM2006)</arxiv:journal_ref>\n    <author>\n      <name>Yousof Al-Hammadi</name>\n    </author>\n    <author>\n      <name>Uwe Aickelin</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/1602.07396v1</id>\n    <title>Concentration of research funding leads to decreasing marginal returns</title>\n    <updated>2016-02-24T04:37:19Z</updated>\n    <link href=\"https://arxiv.org/abs/1602.07396v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/1602.07396v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>In most countries, basic research is supported by research councils that select, after peer review, the individuals or teams that are to receive funding. Unfortunately, the number of grants these research councils can allocate is not infinite and, in most cases, a minority of the researchers receive the majority of the funds. However, evidence as to whether this is an optimal way of distributing available funds is mixed. The purpose of this study is to measure the relation between the amount of funding provided to 12,720 researchers in Quebec over a fifteen year period (1998-2012) and their scientific output and impact from 2000 to 2013. Our results show that both in terms of the quantity of papers produced and of their scientific impact, the concentration of research funding in the hands of a so-called \"elite\" of researchers generally produces diminishing marginal returns. Also, we find that the most funded researchers do not stand out in terms of output and scientific impact.</summary>\n    <category term=\"cs.DL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2016-02-24T04:37:19Z</published>\n    <arxiv:comment>Accepted for publication in Research Evaluation</arxiv:comment>\n    <arxiv:primary_category term=\"cs.DL\"/>\n    <arxiv:journal_ref>Research Evaluation 25 (4): 396-404 (2016)</arxiv:journal_ref>\n    <author>\n      <name>Philippe Mongeon</name>\n    </author>\n    <author>\n      <name>Christine Brodeur</name>\n    </author>\n    <author>\n      <name>Catherine Beaudry</name>\n    </author>\n    <author>\n      <name>Vincent Lariviere</name>\n    </author>\n    <arxiv:doi>10.1093/reseval/rvw007</arxiv:doi>\n    <link rel=\"related\" href=\"https://doi.org/10.1093/reseval/rvw007\" title=\"doi\"/>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2107.10469v2</id>\n    <title>What Makes Sound Event Localization and Detection Difficult? Insights from Error Analysis</title>\n    <updated>2021-10-02T05:29:25Z</updated>\n    <link href=\"https://arxiv.org/abs/2107.10469v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2107.10469v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Sound event localization and detection (SELD) is an emerging research topic that aims to unify the tasks of sound event detection and direction-of-arrival estimation. As a result, SELD inherits the challenges of both tasks, such as noise, reverberation, interference, polyphony, and non-stationarity of sound sources. Furthermore, SELD often faces an additional challenge of assigning correct correspondences between the detected sound classes and directions of arrival to multiple overlapping sound events. Previous studies have shown that unknown interferences in reverberant environments often cause major degradation in the performance of SELD systems. To further understand the challenges of the SELD task, we performed a detailed error analysis on two of our SELD systems, which both ranked second in the team category of DCASE SELD Challenge, one in 2020 and one in 2021. Experimental results indicate polyphony as the main challenge in SELD, due to the difficulty in detecting all sound events of interest. In addition, the SELD systems tend to make fewer errors for the polyphonic scenario that is dominant in the training set.</summary>\n    <category term=\"eess.AS\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.SD\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"eess.SP\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2021-07-22T06:01:49Z</published>\n    <arxiv:comment>Accepted for the 6th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE), 2021</arxiv:comment>\n    <arxiv:primary_category term=\"eess.AS\"/>\n    <arxiv:journal_ref>Proceedings of the Detection and Classification of Acoustic Scenes and Events 2021 Workshop, pp. 120-124</arxiv:journal_ref>\n    <author>\n      <name>Thi Ngoc Tho Nguyen</name>\n    </author>\n    <author>\n      <name>Karn N. Watcharasupat</name>\n    </author>\n    <author>\n      <name>Zhen Jian Lee</name>\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_24","type":"reasoning","text":"**Refining paper selection strategy**\n\nI'm shifting from a broad query to a focused set of manageable papers centered on scientific LLMs and literature-based discovery, targeting well-known or highly relevant works like GAPformer and recent surveys. Next, I'll verify the accessibility and relevance of these papers with precise searches."}}
{"type":"item.started","item":{"id":"item_25","type":"command_execution","command":"/bin/bash -lc \"curl -s 'https://export.arxiv.org/api/query?search_query=all:literature%20based%20discovery%20language%20model&start=0&max_results=5' | sed -n '1,80p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_25","type":"command_execution","command":"/bin/bash -lc \"curl -s 'https://export.arxiv.org/api/query?search_query=all:literature%20based%20discovery%20language%20model&start=0&max_results=5' | sed -n '1,80p'\"","aggregated_output":"<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/kGOwkoaJFyL7BO6h2VFmjgMFWcA</id>\n  <title>arXiv Query: search_query=all:literature OR all:based OR all:discovery OR all:language OR all:model&amp;id_list=&amp;start=0&amp;max_results=5</title>\n  <updated>2025-12-15T01:15:55Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=all:literature+OR+(all:based+OR+(all:discovery+OR+(all:language+OR+all:model)))&amp;start=0&amp;max_results=5&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>5</opensearch:itemsPerPage>\n  <opensearch:totalResults>1562391</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/2411.18583v1</id>\n    <title>Automated Literature Review Using NLP Techniques and LLM-Based Retrieval-Augmented Generation</title>\n    <updated>2024-11-27T18:27:07Z</updated>\n    <link href=\"https://arxiv.org/abs/2411.18583v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2411.18583v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>This research presents and compares multiple approaches to automate the generation of literature reviews using several Natural Language Processing (NLP) techniques and retrieval-augmented generation (RAG) with a Large Language Model (LLM). The ever-increasing number of research articles provides a huge challenge for manual literature review. It has resulted in an increased demand for automation. Developing a system capable of automatically generating the literature reviews from only the PDF files as input is the primary objective of this research work. The effectiveness of several Natural Language Processing (NLP) strategies, such as the frequency-based method (spaCy), the transformer model (Simple T5), and retrieval-augmented generation (RAG) with Large Language Model (GPT-3.5-turbo), is evaluated to meet the primary objective. The SciTLDR dataset is chosen for this research experiment and three distinct techniques are utilized to implement three different systems for auto-generating the literature reviews. The ROUGE scores are used for the evaluation of all three systems. Based on the evaluation, the Large Language Model GPT-3.5-turbo achieved the highest ROUGE-1 score, 0.364. The transformer model comes in second place and spaCy is at the last position. Finally, a graphical user interface is created for the best system based on the large language model.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.IR\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2024-11-27T18:27:07Z</published>\n    <arxiv:comment>Key Words : T5, SpaCy, Large Language Model, GPT, ROUGE, Literature Review, Natural Language Processing, Retrieval-augmented generation</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <author>\n      <name>Nurshat Fateh Ali</name>\n    </author>\n    <author>\n      <name>Md. Mahdi Mohtasim</name>\n    </author>\n    <author>\n      <name>Shakil Mosharrof</name>\n    </author>\n    <author>\n      <name>T. Gopi Krishna</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2503.21676v2</id>\n    <title>How do language models learn facts? Dynamics, curricula and hallucinations</title>\n    <updated>2025-07-24T14:04:20Z</updated>\n    <link href=\"https://arxiv.org/abs/2503.21676v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2503.21676v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Large language models accumulate vast knowledge during pre-training, yet the dynamics governing this acquisition remain poorly understood. This work investigates the learning dynamics of language models on a synthetic factual recall task, uncovering three key findings: First, language models learn in three phases, exhibiting a performance plateau before acquiring precise factual knowledge. Mechanistically, this plateau coincides with the formation of attention-based circuits that support recall. Second, the training data distribution significantly impacts learning dynamics, as imbalanced distributions lead to shorter plateaus. Finally, hallucinations emerge simultaneously with knowledge, and integrating new knowledge into the model through fine-tuning is challenging, as it quickly corrupts its existing parametric memories. Our results emphasize the importance of data distribution in knowledge acquisition and suggest novel data scheduling strategies to accelerate neural network training.</summary>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-03-27T16:43:45Z</published>\n    <arxiv:comment>Accepted at the 2nd Conference on Language Modeling (2025)</arxiv:comment>\n    <arxiv:primary_category term=\"cs.CL\"/>\n    <arxiv:journal_ref>Conference on Language Modeling (2025)</arxiv:journal_ref>\n    <author>\n      <name>Nicolas Zucchet</name>\n    </author>\n    <author>\n      <name>Jörg Bornschein</name>\n    </author>\n    <author>\n      <name>Stephanie Chan</name>\n    </author>\n    <author>\n      <name>Andrew Lampinen</name>\n    </author>\n    <author>\n      <name>Razvan Pascanu</name>\n    </author>\n    <author>\n      <name>Soham De</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2409.02228v1</id>\n    <title>Unforgettable Generalization in Language Models</title>\n    <updated>2024-09-03T18:55:54Z</updated>\n    <link href=\"https://arxiv.org/abs/2409.02228v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2409.02228v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>When language models (LMs) are trained to forget (or \"unlearn'') a skill, how precisely does their behavior change? We study the behavior of transformer LMs in which tasks have been forgotten via fine-tuning on randomized labels. Such LMs learn to generate near-random predictions for individual examples in the \"training'' set used for forgetting. Across tasks, however, LMs exhibit extreme variability in whether LM predictions change on examples outside the training set. In some tasks (like entailment classification), forgetting generalizes robustly, and causes models to produce uninformative predictions on new task instances; in other tasks (like physical commonsense reasoning and scientific question answering) forgetting affects only the training examples, and models continue to perform the \"forgotten'' task accurately even for examples very similar to those that appeared in the training set. Dataset difficulty is not predictive of whether a behavior can be forgotten; instead, generalization in forgetting is (weakly) predicted by the confidence of LMs' initial task predictions and the variability of LM representations of training data, with low confidence and low variability both associated with greater generalization. Perhaps most surprisingly, random-label forgetting appears to be somewhat insensitive to the contents of the training set: for example, models trained on science questions with random labels continue to answer other science questions accurately, but begin to produce random labels on entailment classification tasks. Finally, we show that even generalizable forgetting is shallow: linear probes trained on LMs' representations can still perform tasks reliably after forgetting. Our results highlight the difficulty and unpredictability of performing targeted skill removal from models via fine-tuning.</summary>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2024-09-03T18:55:54Z</published>\n    <arxiv:comment>18 pages, 9 figures, published in First Conference on Language Modeling 2024</arxiv:comment>\n    <arxiv:primary_category term=\"cs.LG\"/>\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_26","type":"reasoning","text":"**Selecting key research papers**\n\nI'm narrowing down to about six relevant papers focusing on scientific LLM surveys, literature review agents, agent training, and autonomous research agents, while considering inclusion of work on research agendas and open AI-for-science challenges."}}
{"type":"item.started","item":{"id":"item_27","type":"command_execution","command":"/bin/bash -lc \"curl -s 'https://export.arxiv.org/api/query?search_query=all:agents4science&start=0&max_results=2' | sed -n '1,60p'\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_27","type":"command_execution","command":"/bin/bash -lc \"curl -s 'https://export.arxiv.org/api/query?search_query=all:agents4science&start=0&max_results=2' | sed -n '1,60p'\"","aggregated_output":"<?xml version='1.0' encoding='UTF-8'?>\n<feed xmlns:opensearch=\"http://a9.com/-/spec/opensearch/1.1/\" xmlns:arxiv=\"http://arxiv.org/schemas/atom\" xmlns=\"http://www.w3.org/2005/Atom\">\n  <id>https://arxiv.org/api/8oVU+F0f9GSKQNyFWaNbhVO8jRk</id>\n  <title>arXiv Query: search_query=all:agents4science&amp;id_list=&amp;start=0&amp;max_results=2</title>\n  <updated>2025-12-15T01:16:02Z</updated>\n  <link href=\"https://arxiv.org/api/query?search_query=all:agents4science&amp;start=0&amp;max_results=2&amp;id_list=\" type=\"application/atom+xml\"/>\n  <opensearch:itemsPerPage>2</opensearch:itemsPerPage>\n  <opensearch:totalResults>6</opensearch:totalResults>\n  <opensearch:startIndex>0</opensearch:startIndex>\n  <entry>\n    <id>http://arxiv.org/abs/2511.15534v1</id>\n    <title>Exploring the use of AI authors and reviewers at Agents4Science</title>\n    <updated>2025-11-19T15:32:07Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.15534v1\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.15534v1\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>There is growing interest in using AI agents for scientific research, yet fundamental questions remain about their capabilities as scientists and reviewers. To explore these questions, we organized Agents4Science, the first conference in which AI agents serve as both primary authors and reviewers, with humans as co-authors and co-reviewers. Here, we discuss the key learnings from the conference and their implications for human-AI collaboration in science.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-19T15:32:07Z</published>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Federico Bianchi</name>\n    </author>\n    <author>\n      <name>Owen Queen</name>\n    </author>\n    <author>\n      <name>Nitya Thakkar</name>\n    </author>\n    <author>\n      <name>Eric Sun</name>\n    </author>\n    <author>\n      <name>James Zou</name>\n    </author>\n  </entry>\n  <entry>\n    <id>http://arxiv.org/abs/2511.04583v2</id>\n    <title>Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper</title>\n    <updated>2025-11-10T15:05:28Z</updated>\n    <link href=\"https://arxiv.org/abs/2511.04583v2\" rel=\"alternate\" type=\"text/html\"/>\n    <link href=\"https://arxiv.org/pdf/2511.04583v2\" rel=\"related\" type=\"application/pdf\" title=\"pdf\"/>\n    <summary>Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, and iteratively conducts experiments until improvements are realized, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. Through our experiments, the Jr. AI Scientist successfully generated new research papers that build upon real NeurIPS, IJCV, and ICLR works by proposing and implementing novel methods. For evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless, we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating the potential risks of directly applying current AI Scientist systems and key challenges for future research. Finally, we comprehensively report various risks identified during development. We believe this study clarifies the current role and limitations of AI Scientist systems, offering insights into the areas that still require human expertise and the risks that may emerge as these systems evolve.</summary>\n    <category term=\"cs.AI\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CL\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.CV\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <category term=\"cs.LG\" scheme=\"http://arxiv.org/schemas/atom\"/>\n    <published>2025-11-06T17:37:49Z</published>\n    <arxiv:comment>Issues, comments, and questions are all welcome in https://github.com/Agent4Science-UTokyo/Jr.AI-Scientist</arxiv:comment>\n    <arxiv:primary_category term=\"cs.AI\"/>\n    <author>\n      <name>Atsuyuki Miyai</name>\n    </author>\n    <author>\n      <name>Mashiro Toyooka</name>\n    </author>\n    <author>\n      <name>Takashi Otonari</name>\n    </author>\n    <author>\n      <name>Zaiying Zhao</name>\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_28","type":"reasoning","text":"**Selecting key recent papers**\n\nIdentified 6 accessible, relevant papers from 2023-2025 for download to build dataset, including topics on scientific LLMs, AI literature review, and AI scientist agents. Next, I'll confirm directory before downloading PDFs."}}
{"type":"item.started","item":{"id":"item_29","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_29","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/llms-expose-science-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_30","type":"reasoning","text":"**Downloading PDFs to papers directory**"}}
{"type":"item.started","item":{"id":"item_31","type":"command_execution","command":"/bin/bash -lc 'wget -q -O papers/2406.10833v3_scientific_llms_survey.pdf https://arxiv.org/pdf/2406.10833v3'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_31","type":"command_execution","command":"/bin/bash -lc 'wget -q -O papers/2406.10833v3_scientific_llms_survey.pdf https://arxiv.org/pdf/2406.10833v3'","aggregated_output":"wget: error while loading shared libraries: libcrypto.so.1.1: cannot open shared object file: No such file or directory\n","exit_code":127,"status":"failed"}}
{"type":"item.completed","item":{"id":"item_32","type":"reasoning","text":"**Switching to curl for downloads**"}}
{"type":"item.started","item":{"id":"item_33","type":"command_execution","command":"/bin/bash -lc 'curl -L -o papers/2406.10833v3_scientific_llms_survey.pdf https://arxiv.org/pdf/2406.10833v3'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_33","type":"command_execution","command":"/bin/bash -lc 'curl -L -o papers/2406.10833v3_scientific_llms_survey.pdf https://arxiv.org/pdf/2406.10833v3'","aggregated_output":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 2250k  100 2250k    0     0  4172k      0 --:--:-- --:--:-- --:--:-- 4174k\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_34","type":"reasoning","text":"**Planning additional downloads with curl**"}}
{"type":"item.started","item":{"id":"item_35","type":"command_execution","command":"/bin/bash -lc 'curl -L -o papers/2411.18583v1_automated_lit_review_rag.pdf https://arxiv.org/pdf/2411.18583v1'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_35","type":"command_execution","command":"/bin/bash -lc 'curl -L -o papers/2411.18583v1_automated_lit_review_rag.pdf https://arxiv.org/pdf/2411.18583v1'","aggregated_output":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  649k  100  649k    0     0  9664k      0 --:--:-- --:--:-- --:--:-- 9688k\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_36","type":"reasoning","text":"**Advancing to next paper**"}}
{"type":"item.started","item":{"id":"item_37","type":"command_execution","command":"/bin/bash -lc 'curl -L -o papers/2308.02443v1_ai_lit_review_suite.pdf https://arxiv.org/pdf/2308.02443v1'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_37","type":"command_execution","command":"/bin/bash -lc 'curl -L -o papers/2308.02443v1_ai_lit_review_suite.pdf https://arxiv.org/pdf/2308.02443v1'","aggregated_output":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  502k  100  502k    0     0  3609k      0 --:--:-- --:--:-- --:--:-- 3616k\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_38","type":"reasoning","text":"**Updating to latest version**"}}
{"type":"item.started","item":{"id":"item_39","type":"command_execution","command":"/bin/bash -lc 'curl -L -o papers/2402.11651v2_learning_from_failure_agent_tuning.pdf https://arxiv.org/pdf/2402.11651v2'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_39","type":"command_execution","command":"/bin/bash -lc 'curl -L -o papers/2402.11651v2_learning_from_failure_agent_tuning.pdf https://arxiv.org/pdf/2402.11651v2'","aggregated_output":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  571k  100  571k    0     0  11.6M      0 --:--:-- --:--:-- --:--:-- 11.8M\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_40","type":"command_execution","command":"/bin/bash -lc 'curl -L -o papers/2511.04583v2_jr_ai_scientist.pdf https://arxiv.org/pdf/2511.04583v2'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_40","type":"command_execution","command":"/bin/bash -lc 'curl -L -o papers/2511.04583v2_jr_ai_scientist.pdf https://arxiv.org/pdf/2511.04583v2'","aggregated_output":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r 76 5334k   76 4096k    0     0  4618k      0  0:00:01 --:--:--  0:00:01 4617k\r100 5334k  100 5334k    0     0  5754k      0 --:--:-- --:--:-- --:--:-- 5748k\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_41","type":"command_execution","command":"/bin/bash -lc 'curl -L -o papers/2511.15534v1_ai_authors_agents4science.pdf https://arxiv.org/pdf/2511.15534v1'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_41","type":"command_execution","command":"/bin/bash -lc 'curl -L -o papers/2511.15534v1_ai_authors_agents4science.pdf https://arxiv.org/pdf/2511.15534v1'","aggregated_output":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 3112k  100 3112k    0     0  5550k      0 --:--:-- --:--:-- --:--:-- 5548k\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_42","type":"reasoning","text":"**Planning stepwise paper processing**\n\nI'm preparing to list paper names and then process them one by one, extracting abstracts and notes into a literature review markdown file while creating a papers README for organization. I'll first check pdfplumber availability and extract the first few pages of paper 2406.10833v3 using Python."}}
{"type":"item.started","item":{"id":"item_43","type":"command_execution","command":"/bin/bash -lc \"python - <<'PY'\nimport pdfplumber, textwrap\npath='papers/2406.10833v3_scientific_llms_survey.pdf'\nwith pdfplumber.open(path) as pdf:\n    text=''.join((page.extract_text() or '')+'\\\\n' for page in pdf.pages[:3])\nprint(textwrap.shorten(text, 4000))\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_43","type":"command_execution","command":"/bin/bash -lc \"python - <<'PY'\nimport pdfplumber, textwrap\npath='papers/2406.10833v3_scientific_llms_survey.pdf'\nwith pdfplumber.open(path) as pdf:\n    text=''.join((page.extract_text() or '')+'\\\\n' for page in pdf.pages[:3])\nprint(textwrap.shorten(text, 4000))\nPY\"","aggregated_output":"Traceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nModuleNotFoundError: No module named 'pdfplumber'\n","exit_code":1,"status":"failed"}}
{"type":"item.completed","item":{"id":"item_44","type":"reasoning","text":"**Checking for alternative PDF libraries**\n\nI'm verifying if PyPDF2 (now pypdf) is available as a fallback since pdfplumber is not installed."}}
{"type":"item.started","item":{"id":"item_45","type":"command_execution","command":"/bin/bash -lc \"python - <<'PY'\nimport pypdf, textwrap\npath='papers/2406.10833v3_scientific_llms_survey.pdf'\nreader=pypdf.PdfReader(path)\ntext=''.join(reader.pages[i].extract_text() or '' for i in range(min(3,len(reader.pages))))\nprint(textwrap.shorten(text, 4000))\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_45","type":"command_execution","command":"/bin/bash -lc \"python - <<'PY'\nimport pypdf, textwrap\npath='papers/2406.10833v3_scientific_llms_survey.pdf'\nreader=pypdf.PdfReader(path)\ntext=''.join(reader.pages[i].extract_text() or '' for i in range(min(3,len(reader.pages))))\nprint(textwrap.shorten(text, 4000))\nPY\"","aggregated_output":"Traceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nModuleNotFoundError: No module named 'pypdf'\n","exit_code":1,"status":"failed"}}
{"type":"item.completed","item":{"id":"item_46","type":"reasoning","text":"**Checking for PyPDF2 availability**"}}
{"type":"item.started","item":{"id":"item_47","type":"command_execution","command":"/bin/bash -lc \"python - <<'PY'\nimport PyPDF2, textwrap\npath='papers/2406.10833v3_scientific_llms_survey.pdf'\nreader=PyPDF2.PdfReader(path)\ntext=''.join(reader.pages[i].extract_text() or '' for i in range(min(3,len(reader.pages))))\nprint(textwrap.shorten(text, 4000))\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_47","type":"command_execution","command":"/bin/bash -lc \"python - <<'PY'\nimport PyPDF2, textwrap\npath='papers/2406.10833v3_scientific_llms_survey.pdf'\nreader=PyPDF2.PdfReader(path)\ntext=''.join(reader.pages[i].extract_text() or '' for i in range(min(3,len(reader.pages))))\nprint(textwrap.shorten(text, 4000))\nPY\"","aggregated_output":"Traceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nModuleNotFoundError: No module named 'PyPDF2'\n","exit_code":1,"status":"failed"}}
{"type":"item.completed","item":{"id":"item_48","type":"reasoning","text":"**Deciding to install PyPDF2**\n\nI'll install PyPDF2 since it's a small library allowed for use, enabling easier PDF processing in the environment."}}
{"type":"item.started","item":{"id":"item_49","type":"command_execution","command":"/bin/bash -lc 'python -m pip install --quiet PyPDF2'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_49","type":"command_execution","command":"/bin/bash -lc 'python -m pip install --quiet PyPDF2'","aggregated_output":"/data/haokunliu/idea-explorer/.venv/bin/python: No module named pip\n","exit_code":1,"status":"failed"}}
{"type":"item.completed","item":{"id":"item_50","type":"reasoning","text":"**Testing alternative PDF text extraction**\n\nI'm trying `pdftotext` as an alternative since pip installations aren't possible, checking if it's available on the system."}}
{"type":"item.started","item":{"id":"item_51","type":"command_execution","command":"/bin/bash -lc 'pdftotext -layout papers/2406.10833v3_scientific_llms_survey.pdf - | head'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_51","type":"command_execution","command":"/bin/bash -lc 'pdftotext -layout papers/2406.10833v3_scientific_llms_survey.pdf - | head'","aggregated_output":"                                              A Comprehensive Survey of Scientific Large Language Models and\n                                                        Their Applications in Scientific Discovery\n                                                                   Yu Zhang♣∗ , Xiusi Chen♢♣∗ , Bowen Jin♣∗ ,\n                                                           Sheng Wang♡ , Shuiwang Ji♠ , Wei Wang♢ , Jiawei Han♣\n                                             ♣\n                                               University of Illinois at Urbana-Champaign ♢ University of California, Los Angeles\n                                                         ♡\n                                                           University of Washington, Seattle ♠ Texas A&M University\n                                                  {yuz9,bowenj4,hanj}@illinois.edu {xchen,weiwang}@cs.ucla.edu\n                                                                  swang@cs.washington.edu sji@tamu.edu\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_52","type":"reasoning","text":"**Planning text extraction via pdftotext**\n\nI'm deciding to use pdftotext to extract the first 120 lines of a PDF for note summaries, possibly piping to sed for filtering. This approach aims to capture abstracts or key sections efficiently."}}
{"type":"item.started","item":{"id":"item_53","type":"command_execution","command":"/bin/bash -lc 'pdftotext -layout papers/2406.10833v3_scientific_llms_survey.pdf - | head -n 80'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_53","type":"command_execution","command":"/bin/bash -lc 'pdftotext -layout papers/2406.10833v3_scientific_llms_survey.pdf - | head -n 80'","aggregated_output":"                                              A Comprehensive Survey of Scientific Large Language Models and\n                                                        Their Applications in Scientific Discovery\n                                                                   Yu Zhang♣∗ , Xiusi Chen♢♣∗ , Bowen Jin♣∗ ,\n                                                           Sheng Wang♡ , Shuiwang Ji♠ , Wei Wang♢ , Jiawei Han♣\n                                             ♣\n                                               University of Illinois at Urbana-Champaign ♢ University of California, Los Angeles\n                                                         ♡\n                                                           University of Washington, Seattle ♠ Texas A&M University\n                                                  {yuz9,bowenj4,hanj}@illinois.edu {xchen,weiwang}@cs.ucla.edu\n                                                                  swang@cs.washington.edu sji@tamu.edu\n\n                                                                Abstract                              Given the broad and profound impact of LLMs\n                                                                                                   in various scientific fields across diverse modalities,\n                                             In many scientific fields, large language mod-        it becomes necessary to comprehensively review\n                                             els (LLMs) have revolutionized the way text           related work in this direction. However, existing\narXiv:2406.10833v3 [cs.CL] 28 Sep 2024\n\n\n\n\n                                             and other modalities of data (e.g., molecules         scientific LLM surveys typically focus on either\n                                             and proteins) are handled, achieving supe-\n                                                                                                   one or two fields (e.g., biomedicine (Wang et al.,\n                                             rior performance in various applications and\n                                             augmenting the scientific discovery process.          2023a; He et al., 2024b; Pei et al., 2024; Zhang\n                                             Nevertheless, previous surveys on scientific          et al., 2024d) and chemistry (Xia et al., 2023; Pei\n                                             LLMs often concentrate on one or two fields           et al., 2024; Zhang et al., 2024d)) or one modality\n                                             or a single modality. In this paper, we aim           (e.g., text (Ho et al., 2024)) only. In fact, if we take\n                                             to provide a more holistic view of the re-            a holistic view of the research landscape, we can\n                                             search landscape by unveiling cross-field and         observe similar and interrelated techniques used to\n                                             cross-modal connections between scientific\n                                                                                                   develop LLMs for different fields and modalities.\n                                             LLMs regarding their architectures and pre-\n                                             training techniques. To this end, we compre-              Figure 1 depicts three major types of scien-\n                                             hensively survey over 260 scientific LLMs,\n                                                                                                   tific LLM pre-training strategies (i.e., C OLUMNS\n                                             discuss their commonalities and differences,\n                                             as well as summarize pre-training datasets            1 to 3), for each of which we give 4 examples\n                                             and evaluation tasks for each field and modal-        (i.e., T YPES A to D). In C OLUMN 1, follow-\n                                             ity. Moreover, we investigate how LLMs                ing BERT (Devlin et al., 2019) and RoBERTa\n                                             have been deployed to benefit scientific discov-      (Liu et al., 2019), existing studies use masked lan-\n                                             ery. Resources related to this survey are avail-      guage modeling (MLM) to pre-train encoder lan-\n                                             able at https://github.com/yuzhimanhua/               guage models. Here, the input can be naturally\n                                             Awesome-Scientific-Language-Models.                   sequential (e.g., papers in each field; protein, DNA,\n                                                                                                   and RNA sequences in the FASTA format (Lip-\n                                         1   Introduction                                          man and Pearson, 1985)) or artificially linearized\n                                         The emergence of large language models (LLMs)             (e.g., molecules in the SMILES format (Weininger,\n                                         (Zhao et al., 2023c) brings a new paradigm to nat-        1988); sequences of venue, author, and reference\n                                         ural language processing (NLP) by replacing spe-          nodes in citation graphs). In C OLUMN 2, inspired\n                                         cialized models designed for each task with unified       by GPT (Brown et al., 2020) and LLaMA (Touvron\n                                         models that are reasonably effective for a wide           et al., 2023a), previous studies adopt next token\n                                         spectrum of problems. In the scientific domain,           prediction to pre-train (encoder-)decoder language\n                                         such a paradigm not only reshapes people’s strate-        models, some of which further adopt instruction\n                                         gies to handle tasks related to natural language (e.g.,   tuning and preference optimization (Ouyang et al.,\n                                         scientific papers, medical records, and climate re-       2022). Other than plain text input (e.g., question-\n                                         ports) but also inspires analogous ideas to deal with     answer pairs from knowledge bases or exams), we\n                                         other types of data (e.g., molecules, proteins, tables,   see more ways to sequentialize complex scientific\n                                         and metadata). In addition to understanding exist-        data, such as flattening table cells and using par-\n                                         ing scientific data, LLMs have shown their poten-         ticle coordinates to describe crystals. Even for\n                                         tial to accelerate scientific discovery (Wang et al.,     images, there are studies in both mathematics (Gao\n                                         2023c; Zhang et al., 2023e; Wang et al., 2024d)           et al., 2023) and biomedicine (Li et al., 2023a)\n                                         through generation, planning, etc.                        that exploit a vision encoder to project an image\n                                                                                                   onto several visual tokens and prepend them to text\n                                             ∗ Equal contribution                                  tokens as linearized LLM input. In C OLUMN 3,\n\f               penicillin                  cataclasite, mylonite, fault breccia                                                relevant\n\n\n                  Encoder                              (Encoder-)Decoder                             Encoder                                            Encoder\n              Language Model                             Language Model                          Language Model                                     Language Model\n\n\n a case of oral [MASK] anaphylaxis is        What kind of rock belongs to               language models are few-shot             cite     pre-training of deep bidirectional\n                                             dynamic metamorphic rocks?                          learners …                                        transformers …\n\n\n (Type 1.A) SciBERT, BioBERT                (Type 2.A) Galactica, K2                                                 (Type 3.A) SPECTER\n                                  matter                   5.0                                                                 relevant\n\n\n\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_54","type":"reasoning","text":"**Planning literature review structure**\n\nI'm preparing to create a literature_review.md with sections summarizing each paper's key contributions, starting by extracting abstracts from PDFs like 2411.18583 to inform concise manual summaries."}}
{"type":"item.started","item":{"id":"item_55","type":"command_execution","command":"/bin/bash -lc 'pdftotext -layout papers/2411.18583v1_automated_lit_review_rag.pdf - | head -n 40'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_55","type":"command_execution","command":"/bin/bash -lc 'pdftotext -layout papers/2411.18583v1_automated_lit_review_rag.pdf - | head -n 40'","aggregated_output":"       Automated Literature Review Using NLP\n   Techniques and LLM-Based Retrieval-Augmented\n                     Generation\n                         Nurshat Fateh Ali                                            Md. Mahdi Mohtasim\n        Department of Computer Science and Engineering                  Department of Computer Science and Engineering\n          Military Institute of Science and Technology                    Military Institute of Science and Technology\n                       Dhaka, Bangladesh                                               Dhaka, Bangladesh\n                     nurshatfateh@gmail.com                                       mahdimohtasim@gmail.com\n\n                          Shakil Mosharrof                                                T. Gopi Krishna\n        Department of Computer Science and Engineering                   Department of Computer Science and Engineering\n          Military Institute of Science and Technology                     Military Institute of Science and Technology\n                       Dhaka, Bangladesh                                                Dhaka, Bangladesh\n                     shakilmrf8@gmail.com                                             gopi.mistbd@gmail.com\n\n\n\n\n   Abstract—This research presents and compares multiple ap-          relevant information can be a time-consuming, tedious, and\nproaches to automate the generation of literature reviews using       error-prone task. Due to these difficulties, there has been\nseveral Natural Language Processing (NLP) techniques and              an increasing interest in automating the process of literature\nretrieval-augmented generation (RAG) with a Large Language\nModel (LLM). The ever-increasing number of research articles          reviews [1]. Automated systems can use natural language\nprovides a huge challenge for manual literature review. It has        processing techniques and machine learning algorithms to\nresulted in an increased demand for automation. Developing a          analyze extensive amounts of text, extract relevant details, and\nsystem capable of automatically generating the literature reviews     create structured summaries [2].\nfrom only the PDF files as input is the primary objective of this        The primary objective of this research is to develop a system\nresearch work. The effectiveness of several Natural Language\nProcessing (NLP) strategies, such as the frequency-based method       that can automatically generate the literature review segment\n(spaCy), the transformer model (Simple T5), and retrieval-            of a research paper by using only the PDF files of the related\naugmented generation (RAG) with Large Language Model (GPT-            papers as input. Several Natural Language Processing tech-\n3.5-turbo), is evaluated to meet the primary objective. The           niques such as the Frequency-based approach, Transformer-\nSciTLDR dataset is chosen for this research experiment and            based approach, and Large Language Model-based approach\nthree distinct techniques are utilized to implement three different\nsystems for auto-generating the literature reviews. The ROUGE         are implemented and compared to find the best procedure. The\nscores are used for the evaluation of all three systems. Based        SciTLDR dataset [3] is selected for this research work. The\non the evaluation, the Large Language Model GPT-3.5-turbo             first procedure uses the frequency-based approach. The library\nachieved the highest ROUGE-1 score, 0.364. The transformer            named spaCy [4] is utilized here. The second procedure uses\nmodel comes in second place and spaCy is at the last position.        the transformer-based model. The Simple T5 model is utilized\nFinally, a graphical user interface is created for the best system\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_56","type":"reasoning","text":"**Advancing to next document**"}}
{"type":"item.started","item":{"id":"item_57","type":"command_execution","command":"/bin/bash -lc 'pdftotext -layout papers/2308.02443v1_ai_lit_review_suite.pdf - | head -n 40'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_57","type":"command_execution","command":"/bin/bash -lc 'pdftotext -layout papers/2308.02443v1_ai_lit_review_suite.pdf - | head -n 40'","aggregated_output":"                                                                       AI L ITERATURE R EVIEW S UITE\n\n\n                                                                                           David A. Tovar\n                                                                                       Department of Psychology\n                                                                                         Vanderbilt University\n                                                                                            Nashville, TN\n                                                                                    david.tovar@vanderbilt.edu\narXiv:2308.02443v1 [cs.DL] 27 Jul 2023\n\n\n\n\n                                                                                              A BSTRACT\n                                                  The process of conducting literature reviews is often time-consuming and labor-intensive. To\n                                                  streamline this process, I present an AI Literature Review Suite that integrates several functionalities\n                                                  to provide a comprehensive literature review. This tool leverages the power of open access science,\n                                                  large language models (LLMs) and natural language processing to enable the searching, downloading,\n                                                  and organizing of PDF files, as well as extracting content from articles. Semantic search queries\n                                                  are used for data retrieval, while text embeddings and summarization using LLMs present succinct\n                                                  literature reviews. Interaction with PDFs is enhanced through a user-friendly graphical user interface\n                                                  (GUI). The suite also features integrated programs for bibliographic organization, interaction and\n                                                  query, and literature review summaries. This tool presents a robust solution to automate and optimize\n                                                  the process of literature review in academic and industrial research.\n\n                                         Keywords Literature Review · Artificial Intelligence · Text Embeddings · Large Language Models\n\n\n                                         1   Introduction\n\n                                         In academic and industry research, literature reviews serve as the cornerstone of extensive comprehension and ex-\n                                         ploration of any given topic. Traditional manual processes of literature reviews are, however, characterized by\n                                         time-consuming and labor-intensive tasks. These tasks often include sifting through volumes of academic papers,\n                                         manually downloading and organizing relevant ones, reading and summarizing these papers, and finally, synthesizing\n                                         the information into a cohesive narrative. The sheer magnitude of academic papers published daily and the complexity\n                                         of most research topics compound this issue further. Consequently, there has been a growing need for more efficient\n                                         tools that can automate and streamline the literature review process, thereby enabling researchers to focus more on\n                                         knowledge synthesis and less on the logistical aspects of conducting literature reviews.\n                                         To address this need, I introduce the AI Literature Review Suite, a comprehensive suite of integrated programs for\n                                         conducting literature reviews efficiently and accurately. This tool capitalizes on the advancements in machine learning\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_58","type":"reasoning","text":"**Loading next paper ID**"}}
{"type":"item.started","item":{"id":"item_59","type":"command_execution","command":"/bin/bash -lc 'pdftotext -layout papers/2402.11651v2_learning_from_failure_agent_tuning.pdf - | head -n 40'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_59","type":"command_execution","command":"/bin/bash -lc 'pdftotext -layout papers/2402.11651v2_learning_from_failure_agent_tuning.pdf - | head -n 40'","aggregated_output":"                                         Learning From Failure: Integrating Negative Examples when Fine-tuning\n                                                           Large Language Models as Agents\n\n                                                          Renxi Wang1,2   Haonan Li1,2     Xudong Han1,2\n                                                              Yixuan Zhang1,2  Timothy Baldwin1,2,3\n                                                   1               2               3\n                                                     LibrAI          MBZUAI          The University of Melbourne\n                                         {renxi.wang,haonan.li,xudong.han,yixuan.zhang,timothy.baldwin}@mbzuai.ac.ae\n\n\n\n                                                                   Abstract                               (Green Jr et al., 1961; Weizenbaum, 1966). Re-\n                                                                                                          cent work has built off powerful LLMs such as\n                                                 Large language models (LLMs) have achieved\n                                                                                                          GPT-4 (Achiam et al., 2023), using them as the\n                                                 success in acting as agents, which interact with\narXiv:2402.11651v2 [cs.CL] 16 Apr 2024\n\n\n\n\n                                                 environments through tools such as search en-            core of an agent system to process information and\n                                                 gines. However, LLMs are optimized for lan-              make decisions (Gravitas, 2024; Yoheinakajima,\n                                                 guage generation instead of tool use during              2024). This line of work has resulted in agent sys-\n                                                 training or alignment, limiting their effective-         tems that are able to perform much more complex\n                                                 ness as agents. To resolve this problem, previ-          and general tasks.\n                                                 ous work has first collected interaction trajec-            However, these agents generally rely on closed-\n                                                 tories between LLMs and environments, using\n                                                                                                          source LLMs through paid APIs, raising concerns\n                                                 only trajectories that successfully finished the\n                                                 task to fine-tune smaller models, making fine-           about cost, latency, and reproducibility. Addition-\n                                                 tuning data scarce and acquiring it both diffi-          ally, existing LLMs were not developed for agent\n                                                 cult and costly. Discarding failed trajectories          use cases (e.g., generating actions or calling tools),\n                                                 also leads to significant wastage of data and            and few-shot prompting offers only limited learn-\n                                                 resources and limits the possible optimization           ing support (Chen et al., 2023).\n                                                 paths during fine-tuning. In this paper, we ar-             Subsequent work has explored fine-tuning LLMs\n                                                 gue that unsuccessful trajectories offer valuable        as agents, typically in three stages: data collection,\n                                                 insights, and LLMs can learn from these trajec-\n                                                                                                          fine-tuning, and inference (Chen et al., 2023; Zeng\n                                                 tories through appropriate quality control and\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_60","type":"command_execution","command":"/bin/bash -lc 'pdftotext -layout papers/2511.04583v2_jr_ai_scientist.pdf - | head -n 40'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_60","type":"command_execution","command":"/bin/bash -lc 'pdftotext -layout papers/2511.04583v2_jr_ai_scientist.pdf - | head -n 40'","aggregated_output":"                                                                    Jr. AI Scientist and Its Risk Report:\n                                                         Autonomous Scientific Exploration from a Baseline Paper\n\n\n                                                         Atsuyuki Miyai* , Mashiro Toyooka* , Takashi Otonari, Zaiying Zhao,\narXiv:2511.04583v2 [cs.AI] 10 Nov 2025\n\n\n\n\n                                                                                 Kiyoharu Aizawa\n                                                                              The University of Tokyo\n                                                             {miyai, otonari, zhao}@cvm.t.u-tokyo.ac.jp\n                                                                {toyooka, aizawa}@hal.t.u-tokyo.ac.jp\n\n\n                                                        https://github.com/Agent4Science-UTokyo/Jr.AI-Scientist\n\n\n\n\n                                                                                                    Abstract\n\n                                                    Understanding the current capabilities and risks of AI Scientist systems is essential for ensuring trustworthy\n                                                and sustainable AI-driven scientific progress while preserving the integrity of the academic ecosystem. To\n                                                this end, we develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core\n                                                research workflow of a novice student researcher: Given the baseline paper from the human mentor, it analyzes\n                                                its limitations, formulates novel hypotheses for improvement, and iteratively conducts experiments until\n                                                improvements are realized, and writes a paper with the results. Unlike previous approaches that assume\n                                                full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow\n                                                and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically\n                                                valuable contributions. Through our experiments, the Jr. AI Scientist successfully generated new research\n                                                papers that build upon real NeurIPS, IJCV, and ICLR works by proposing and implementing novel methods. For\n                                                evaluation, we conducted automated assessments using AI Reviewers, author-led evaluations, and submissions\n                                                to Agents4Science, a venue dedicated to AI-driven scientific contributions. The findings demonstrate that Jr. AI\n                                                Scientist generates papers receiving higher review scores than existing fully automated systems. Nevertheless,\n                                                we identify important limitations from both the author evaluation and the Agents4Science reviews, indicating\n                                                the potential risks of directly applying current AI Scientist systems and key challenges for future research.\n                                                Finally, we comprehensively report various risks identified during development. We believe this study clarifies\n                                                the current role and limitations of AI Scientist systems, offering insights into the areas that still require human\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_61","type":"command_execution","command":"/bin/bash -lc 'pdftotext -layout papers/2511.15534v1_ai_authors_agents4science.pdf - | head -n 40'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_61","type":"command_execution","command":"/bin/bash -lc 'pdftotext -layout papers/2511.15534v1_ai_authors_agents4science.pdf - | head -n 40'","aggregated_output":"Exploring the use of AI authors and reviewers at Agents4Science\nFederico Bianchi1,4,*, Owen Queen2,4,*, Nitya Thakkar2,4, Eric Sun3,4, James Zou1,2,3,4\n\n1 Together AI\n2 Department of Computer Science, Stanford University\n3 Department of Biomedical Data Science, Stanford University\n4 Co-organizer of Agents4Science Conference\n\n* Equal contribution\nCorrespondence: jamesz@stanford.edu\n\n\n\nAbstract\nThere is growing interest in using AI agents for scientific research, yet fundamental questions remain\nabout their capabilities as scientists and reviewers. To explore these questions, we organized\nAgents4Science, the first conference in which AI agents serve as both primary authors and reviewers,\nwith humans as co-authors and co-reviewers. Here, we discuss the key learnings from the conference and\ntheir implications for human-AI collaboration in science.\n\n\n\nIntroduction\nArtificial intelligence (AI) are no longer just tools for science – they now act as ‘co-scientists’,\nparticipating in all stages of research design and analysis. Traditionally, researchers begin with a well-\ndefined question, such as predicting protein structures from amino acid sequences, and then develop or\napply AI tools (like AlphaFold) to solve that specific problem. Over the past year, researchers have\nincreasingly begun to use AI as a co-scientist to participate in a broader range of scientific activities\nincluding hypothesis generation, experimental design, and paper writing [1-5]. These AI co-scientists are\npowered by advances in AI agents—autonomous systems built on top of large language models (LLMs)\nthat can use existing tools, access external databases, and search through scientific literature.\n\nWhile there are promising examples of AI co-scientists designing nanobodies and generating\nexperimentally validated hypotheses, this remains an emerging frontier [1,2]. Many fundamental\nquestions are still open: How creative are AI scientist agents? How should human researchers collaborate\nwith them? How capable are LLMs at reviewing scientific work? These questions are difficult to study\nbecause journals and conferences currently prohibit AI co-authors and LLM reviewers, and researchers\noften hide how they use AI [5].\n\nTo address this gap, we organized Agents4Science, the first conference where AI agents served as both\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_62","type":"reasoning","text":"**Planning dataset selection and sample creation**\n\nI'm identifying manageable open bibliometric datasets like OpenAlex or arXiv metadata for research, deciding to create a small sample dataset by querying the OpenAlex API for specific topics and saving the results as a JSON file to meet dataset requirements. Next, I'll verify the working directory before downloading."}}
{"type":"item.started","item":{"id":"item_63","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_63","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/llms-expose-science-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_64","type":"reasoning","text":"**Creating sample dataset from OpenAlex**\n\nI'm generating a climate adaptation dataset using a curl command to fetch 200 works from OpenAlex and saving the output to `datasets/openalex_climate_adaptation.json`. Next, I'll create the accompanying README for the dataset."}}
{"type":"item.started","item":{"id":"item_65","type":"command_execution","command":"/bin/bash -lc \"curl -L 'https://api.openalex.org/works?filter=topic.display_name:climate%20adaptation&per_page=200' -o datasets/openalex_climate_adaptation.json\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_65","type":"command_execution","command":"/bin/bash -lc \"curl -L 'https://api.openalex.org/works?filter=topic.display_name:climate%20adaptation&per_page=200' -o datasets/openalex_climate_adaptation.json\"","aggregated_output":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  4707  100  4707    0     0  13497      0 --:--:-- --:--:-- --:--:-- 13525\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_66","type":"reasoning","text":"**Planning second dataset creation**\n\nI'm considering using \"neglected tropical diseases\" as a query to create a second dataset and meet the target of having 1–3 datasets total."}}
{"type":"item.started","item":{"id":"item_67","type":"command_execution","command":"/bin/bash -lc \"curl -L 'https://api.openalex.org/works?filter=topic.display_name:neglected%20tropical%20disease&per_page=200' -o datasets/openalex_neglected_tropical_disease.json\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_67","type":"command_execution","command":"/bin/bash -lc \"curl -L 'https://api.openalex.org/works?filter=topic.display_name:neglected%20tropical%20disease&per_page=200' -o datasets/openalex_neglected_tropical_disease.json\"","aggregated_output":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  4707  100  4707    0     0  49530      0 --:--:-- --:--:-- --:--:-- 50074\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_68","type":"reasoning","text":"**Planning dataset README and repo cloning**\n\nI'm outlining a dataset README that includes using the OpenAlex API for broader topics and advises retrieving more records via cursor. I'll prepare sample extractions from top records, as a small 4KB sample is manageable. For code, I'll clone one or two relevant GitHub repos like the Jr.AI-Scientist as baseline and ensure cloning scripts verify the current directory before cloning."}}
{"type":"item.started","item":{"id":"item_69","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_69","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/llms-expose-science-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_70","type":"reasoning","text":"**Cloning key repositories**\n\nI'm cloning the Jr.AI-Scientist repo into code/jr-ai-scientist and collecting the Awesome-Scientific-Language-Models resource list next, postponing other potential repos for now."}}
{"type":"item.started","item":{"id":"item_71","type":"command_execution","command":"/bin/bash -lc 'git clone https://github.com/Agent4Science-UTokyo/Jr.AI-Scientist.git code/jr-ai-scientist'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_71","type":"command_execution","command":"/bin/bash -lc 'git clone https://github.com/Agent4Science-UTokyo/Jr.AI-Scientist.git code/jr-ai-scientist'","aggregated_output":"Cloning into 'code/jr-ai-scientist'...\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_72","type":"command_execution","command":"/bin/bash -lc 'git clone https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.git code/awesome-scientific-llms'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_72","type":"command_execution","command":"/bin/bash -lc 'git clone https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models.git code/awesome-scientific-llms'","aggregated_output":"Cloning into 'code/awesome-scientific-llms'...\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_73","type":"reasoning","text":"**Listing key code files**\n\nI'm running a directory listing on the code folder to identify key files for inspection and README summary."}}
{"type":"item.started","item":{"id":"item_74","type":"command_execution","command":"/bin/bash -lc 'ls code'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_74","type":"command_execution","command":"/bin/bash -lc 'ls code'","aggregated_output":"awesome-scientific-llms\njr-ai-scientist\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_75","type":"reasoning","text":"**Checking README samples**\n\nI'm reading the first 60 lines of each README to quickly grasp the project's purpose and key details."}}
{"type":"item.started","item":{"id":"item_76","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,60p' code/jr-ai-scientist/README.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_76","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,60p' code/jr-ai-scientist/README.md\"","aggregated_output":"# Jr. AI Scientist and Its Risk Report: Autonomous Scientific Exploration from a Baseline Paper [[arXiv](https://arxiv.org/abs/2511.04583)]\n\n<div align=\"center\">\n<br>\n    <a href='https://atsumiyai.github.io/' target='_blank'>Atsuyuki Miyai*</a>&emsp;\n    Mashiro Toyooka*&emsp;\n    <a href='https://otonari726.github.io/' target='_blank'>Takashi Otonari</a>&emsp;\n    <a href='https://zaiyingzhao.github.io/' target='_blank'>Zaiying Zhao</a>&emsp;\n    <a href='https://scholar.google.co.jp/citations?user=CJRhhi0AAAAJ&hl=en' target='_blank'>Kiyoharu Aizawa</a>&emsp;\n</div>\n<div align=\"center\">\n    The University of Tokyo\n     <br>\n    *Equal Contribution\n    <br>\n</div>\n\n\n<img src=\"figs/iceberg.png\" alt=\"teaser.png\" width=\"100%\">\n\n\n## Backgound\nWe believe that understanding the limits and risks of AI Scientists is essential to ensuring safe and sustainable AI progress.\n\nThe current quality of AI Scientists is still far from sufficient, and how to further improve them remains an open challenge.　In addition, most existing AI Scientist systems have primarily focused on success cases, and do not focus on the risks and failures. This has hindered the academic community to accurately understand AI Scientists.\n\nTo this end, we present the development of a state-of-the-art AI Scientist and provide a comprehensive report on its limitations, risks, and failures, with the goal of fostering a more accurate understanding of AI Scientists within the research community.\n\n\n## Key Summary\n\n- 1. **Development of Jr. AI Scientist.** We develop Jr. AI Scientist, a state-of-the-art autonomous AI scientist system that mimics the core research workflow of an early-career human scientist: Given the baseline paper from the human mentor, it analyzes its limitations, formulates novel hypotheses for improvement, validates them through rigorous experimentation, and writes a paper with the results. Unlike previous approaches that assume full automation or operate on small-scale code, Jr. AI Scientist follows a well-defined research workflow and leverages modern coding agents to handle complex, multi-file implementations, leading to scientifically valuable contributions. \n![workflow.png](figs/workflow.png)\n\n- 2. **Comprehensive Reporting of Risks and Limitations.** We comprehensively report important limitations and various risks identified during development. These include the potential for review-score hacking, difficulties in ensuring proper citation, challenges in managing ablation experiment results, and the problem of detecting fabricated results.\n\nWe hope these insights will deepen understanding of current progress and risks in AI Scientist development.\n\n\n## Generated Papers\nWe created our papers based on [LoCoOp](https://arxiv.org/abs/2306.01293) [NeurIPS2023], [Min-K%++](https://arxiv.org/abs/2404.02936) [ICLR2025], and [GL-MCM](https://arxiv.org/abs/2304.04521) [IJCV2025].\n\n<img src=\"figs/generated-papers.png\" alt=\"generated_papers.png\" width=\"100%\">\n\nEach paper is under `generated_papers/`\n\n## Regarding Open-Sourcing\nWe are currently pausing the public release of Jr. AI Scientist because we have not yet been able to properly assess the potential negative impacts that AI Scientist might have on the academic community.\n\nWe plan to proceed with open-sourcing once we have gained a sufficient understanding of these risks.\n\n\n\n## Community Feedback Form\nWe are collecting issues and feedback from the community through [this form](https://docs.google.com/forms/d/e/1FAIpQLSdbzKqI0bfcdV4gjMYV-gZJzEXKViMOA0MrxEQ0nGAsSxgEzw/viewform?usp=sharing&ouid=103623120947968158097). This is intended to gather feedback from the community regarding issue with the paper, suggestions, and any other inquiries. Submissions can be made either anonymously or with your name.\n\n\n## Citation\nIf you find our work helpful, please consider citing:\n\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_77","type":"reasoning","text":"**Noting missing README**\n\nThe other repository lacks a README file, so I'll plan to add one there for better clarity and onboarding."}}
{"type":"item.started","item":{"id":"item_78","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,40p' code/awesome-scientific-llms/README.md\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_78","type":"command_execution","command":"/bin/bash -lc \"sed -n '1,40p' code/awesome-scientific-llms/README.md\"","aggregated_output":"# Awesome Scientific Language Models\n[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)\n[![Stars](https://img.shields.io/github/stars/yuzhimanhua/Awesome-Scientific-Language-Models?style=social)](https://img.shields.io/github/stars/yuzhimanhua/Awesome-Scientific-Language-Models?style=social)\n\n[![Papers](https://img.shields.io/badge/PaperNumber-296-blue)](https://img.shields.io/badge/PaperNumber-296-blue)\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n[![PRWelcome](https://img.shields.io/badge/PRs-Welcome-red)](https://img.shields.io/badge/PRs-Welcome-red)\n\nA curated list of pre-trained language models in scientific domains (e.g., **mathematics**, **physics**, **chemistry**, **materials science**, **biology**, **medicine**, **geoscience**), covering different model sizes (from **100M** to **100B parameters**) and modalities (e.g., **language**, **graph**, **vision**, **table**, **molecule**, **protein**, **genome**, **climate time series**). \n\nThe repository is part of our survey paper [**A Comprehensive Survey of Scientific Large Language Models and Their Applications in Scientific Discovery**](https://arxiv.org/abs/2406.10833) and will be continuously updated.\n\n**NOTE 1**: To avoid ambiguity, when we talk about the number of parameters in a model, \"Base\" refers to 110M (i.e., BERT-Base), and \"Large\" refers to 340M (i.e., BERT-Large). Other numbers will be written explicitly.\n\n**NOTE 2**: In each subsection, papers are sorted chronologically. If a paper has a preprint (e.g., arXiv or bioRxiv) version, its publication date is according to the preprint service. Otherwise, its publication date is according to the conference proceeding or journal.\n\n**NOTE 3**: We appreciate contributions. If you have any suggested papers, feel free to reach out to yuzhang@tamu.edu or submit a [pull request](https://github.com/yuzhimanhua/Awesome-Scientific-Language-Models/pulls). For format consistency, we will include a paper after (1) it has a version with author names AND (2) its GitHub and/or Hugging Face links are available.\n\n\n\n\n\n<p align=\"center\">\n    <img src=\"intro.svg\" width=\"90%\" style=\"align:center;\"/>\n</p>\n\n\n\n\n\n## Contents\n- [General](#general)\n  - [Language](#general-language)\n  - [Language + Graph](#general-language-graph)\n- [Mathematics](#mathematics)\n  - [Language](#mathematics-language)\n  - [Language + Vision](#mathematics-language-vision)\n  - [Other Modalities (Table)](#mathematics-other-modalities-table)\n- [Physics](#physics)\n  - [Language](#physics-language)\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_79","type":"reasoning","text":"**Planning documentation and summaries**\n\nI'm preparing to create READMEs for papers, datasets, and code, plus a literature review and resource catalog, ensuring concise summaries and sample mentions. Before writing, I'll update the plan statuses to mark step1 complete and advance to step2."}}
{"type":"item.updated","item":{"id":"item_3","type":"todo_list","items":[{"text":"Collect keywords and shortlist target papers (search APIs/web) and download PDFs to papers/","completed":true},{"text":"Identify datasets, download/snapshot small samples, and document dataset instructions in datasets/","completed":false},{"text":"Find and clone relevant code repos to code/ with documentation","completed":false},{"text":"Write literature_review.md and resources.md; create papers/datasets/code READMEs; finalize completion marker","completed":false}]}}
{"type":"item.completed","item":{"id":"item_80","type":"reasoning","text":"**Planning literature and dataset READMEs**\n\nI'm organizing paper metadata into a standardized papers/README covering title, authors, year, arXiv ID, and relevance. I'll create a literature_review file with structured sections including Research Area Overview and Key Papers. For datasets, I'll add a README with OpenAlex usage instructions and license notes, plus a small JSON sample extracted via Python's built-in json module into a new datasets/samples directory."}}
{"type":"item.started","item":{"id":"item_81","type":"command_execution","command":"/bin/bash -lc \"python - <<'PY'\nimport json, pathlib\nfor name in ['openalex_climate_adaptation','openalex_neglected_tropical_disease']:\n    path=pathlib.Path('datasets', f'{name}.json')\n    data=json.loads(path.read_text())\n    results=data.get('results', [])\n    sample=results[:5]\n    out_path=pathlib.Path('datasets', f'{name}_samples.json')\n    out_path.write_text(json.dumps(sample, indent=2))\n    print(f'Wrote {out_path} with {len(sample)} records from {len(results)} total')\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_81","type":"command_execution","command":"/bin/bash -lc \"python - <<'PY'\nimport json, pathlib\nfor name in ['openalex_climate_adaptation','openalex_neglected_tropical_disease']:\n    path=pathlib.Path('datasets', f'{name}.json')\n    data=json.loads(path.read_text())\n    results=data.get('results', [])\n    sample=results[:5]\n    out_path=pathlib.Path('datasets', f'{name}_samples.json')\n    out_path.write_text(json.dumps(sample, indent=2))\n    print(f'Wrote {out_path} with {len(sample)} records from {len(results)} total')\nPY\"","aggregated_output":"Wrote datasets/openalex_climate_adaptation_samples.json with 0 records from 0 total\nWrote datasets/openalex_neglected_tropical_disease_samples.json with 0 records from 0 total\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_82","type":"reasoning","text":"**Checking dataset file contents**"}}
{"type":"item.started","item":{"id":"item_83","type":"command_execution","command":"/bin/bash -lc 'head -n 5 datasets/openalex_climate_adaptation.json'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_83","type":"command_execution","command":"/bin/bash -lc 'head -n 5 datasets/openalex_climate_adaptation.json'","aggregated_output":"{\"error\":\"Invalid query parameters error.\",\"message\":\"topic.display_name is not a valid field. Valid fields are underscore or hyphenated versions of: abstract.search, abstract.search.no_stem, apc_list.currency, apc_list.provenance, apc_list.value, apc_list.value_usd, apc_paid.currency, apc_paid.provenance, apc_paid.value, apc_paid.value_usd, author.id, author.orcid, authors_count, authorships.affiliations.institution_ids, authorships.author.id, authorships.author.orcid, authorships.countries, authorships.institutions.continent, authorships.institutions.country_code, authorships.institutions.id, authorships.institutions.is_global_south, authorships.institutions.lineage, authorships.institutions.ror, authorships.institutions.type, authorships.is_corresponding, awards.doi, awards.funder_award_id, awards.funder_display_name, awards.funder_id, awards.id, best_oa_location.is_accepted, best_oa_location.is_oa, best_oa_location.is_published, best_oa_location.landing_page_url, best_oa_location.license, best_oa_location.license_id, best_oa_location.raw_type, best_oa_location.source.host_organization, best_oa_location.source.host_organization_lineage, best_oa_location.source.id, best_oa_location.source.is_in_doaj, best_oa_location.source.is_oa, best_oa_location.source.issn, best_oa_location.source.type, best_oa_location.version, best_open_version, biblio.first_page, biblio.issue, biblio.last_page, biblio.volume, citation_normalized_percentile.is_in_top_10_percent, citation_normalized_percentile.is_in_top_1_percent, citation_normalized_percentile.value, cited_by, cited_by_count, cited_by_percentile_year.max, cited_by_percentile_year.min, cites, concept.id, concepts.id, concepts.wikidata, concepts_count, corresponding_author_ids, corresponding_institution_ids, countries_distinct_count, created_date, datasets, default.search, display_name, display_name.search, display_name.search.no_stem, doi, doi_starts_with, from_created_date, from_publication_date, fulltext.search, fulltext_origin, funders.id, fwci, has_abstract, has_content.grobid_xml, has_content.pdf, has_doi, has_embeddings, has_fulltext, has_oa_accepted_or_published_version, has_oa_submitted_version, has_old_authors, has_orcid, has_pdf_url, has_pmcid, has_pmid, has_raw_affiliation_strings, has_references, ids.mag, ids.openalex, ids.pmcid, ids.pmid, indexed_in, institution.id, institution_assertions.country_code, institution_assertions.id, institution_assertions.lineage, institution_assertions.ror, institution_assertions.type, institutions.continent, institutions.country_code, institutions.id, institutions.is_global_south, institutions.ror, institutions.type, institutions_distinct_count, is_corresponding, is_oa, is_paratext, is_retracted, is_xpac, journal, keyword.search, keywords.id, language, locations.is_accepted, locations.is_oa, locations.is_published, locations.landing_page_url, locations.license, locations.license_id, locations.raw_type, locations.source.has_issn, locations.source.host_institution_lineage, locations.source.host_organization, locations.source.host_organization_lineage, locations.source.id, locations.source.is_core, locations.source.is_in_doaj, locations.source.is_oa, locations.source.issn, locations.source.publisher_lineage, locations.source.type, locations.version, locations_count, mag, mag_only, oa_status, open_access.any_repository_has_fulltext, open_access.is_oa, open_access.oa_status, openalex, openalex_id, pmcid, pmid, primary_location.is_accepted, primary_location.is_oa, primary_location.is_published, primary_location.landing_page_url, primary_location.license, primary_location.license_id, primary_location.raw_type, primary_location.source.has_issn, primary_location.source.host_institution_lineage, primary_location.source.host_organization, primary_location.source.host_organization_lineage, primary_location.source.id, primary_location.source.is_core, primary_location.source.is_in_doaj, primary_location.source.is_oa, primary_location.source.issn, primary_location.source.publisher_lineage, primary_location.source.type, primary_location.version, primary_topic.domain.id, primary_topic.field.id, primary_topic.id, primary_topic.subfield.id, publication_date, publication_year, raw_affiliation_strings.search, raw_author_name.search, referenced_works, referenced_works_count, related_to, repository, semantic.search, sustainable_development_goals.id, sustainable_development_goals.score, title.search, title.search.no_stem, title_and_abstract.search, title_and_abstract.search.no_stem, to_created_date, to_publication_date, to_updated_date, topics.domain.id, topics.field.id, topics.id, topics.subfield.id, topics_count, type, type_crossref, updated_date, version\"}\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_84","type":"reasoning","text":"**Correcting API query approach**\n\nThe current filter using concept display names is invalid; switching to search with query parameters or filtering by concept IDs is more effective. I'll redo the API requests using the `search` parameter for straightforward queries."}}
{"type":"item.started","item":{"id":"item_85","type":"command_execution","command":"/bin/bash -lc \"curl -L 'https://api.openalex.org/works?search=climate%20adaptation&per_page=200' -o datasets/openalex_climate_adaptation.json\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_85","type":"command_execution","command":"/bin/bash -lc \"curl -L 'https://api.openalex.org/works?search=climate%20adaptation&per_page=200' -o datasets/openalex_climate_adaptation.json\"","aggregated_output":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100 13032    0 13032    0     0  12362      0 --:--:--  0:00:01 --:--:-- 12364\r100 4011k    0 4011k    0     0  3133k      0 --:--:--  0:00:01 --:--:-- 3134k\n","exit_code":0,"status":"completed"}}
{"type":"item.started","item":{"id":"item_86","type":"command_execution","command":"/bin/bash -lc \"curl -L 'https://api.openalex.org/works?search=neglected%20tropical%20disease&per_page=200' -o datasets/openalex_neglected_tropical_disease.json\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_86","type":"command_execution","command":"/bin/bash -lc \"curl -L 'https://api.openalex.org/works?search=neglected%20tropical%20disease&per_page=200' -o datasets/openalex_neglected_tropical_disease.json\"","aggregated_output":"  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  0     0    0     0    0     0      0      0 --:--:--  0:00:01 --:--:--     0\r100 4978k    0 4978k    0     0  2816k      0 --:--:--  0:00:01 --:--:-- 2815k\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_87","type":"reasoning","text":"**Regenerating samples**"}}
{"type":"item.started","item":{"id":"item_88","type":"command_execution","command":"/bin/bash -lc \"python - <<'PY'\nimport json, pathlib\nfor name in ['openalex_climate_adaptation','openalex_neglected_tropical_disease']:\n    path=pathlib.Path('datasets', f'{name}.json')\n    data=json.loads(path.read_text())\n    results=data.get('results', [])\n    sample=results[:5]\n    out_path=pathlib.Path('datasets', f'{name}_samples.json')\n    out_path.write_text(json.dumps(sample, indent=2))\n    print(f'Wrote {out_path} with {len(sample)} records from {len(results)} total')\nPY\"","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_88","type":"command_execution","command":"/bin/bash -lc \"python - <<'PY'\nimport json, pathlib\nfor name in ['openalex_climate_adaptation','openalex_neglected_tropical_disease']:\n    path=pathlib.Path('datasets', f'{name}.json')\n    data=json.loads(path.read_text())\n    results=data.get('results', [])\n    sample=results[:5]\n    out_path=pathlib.Path('datasets', f'{name}_samples.json')\n    out_path.write_text(json.dumps(sample, indent=2))\n    print(f'Wrote {out_path} with {len(sample)} records from {len(results)} total')\nPY\"","aggregated_output":"Wrote datasets/openalex_climate_adaptation_samples.json with 5 records from 200 total\nWrote datasets/openalex_neglected_tropical_disease_samples.json with 5 records from 200 total\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_89","type":"reasoning","text":"**Preparing documentation files**\n\nI'm preparing to create README files for datasets, code, and papers, starting by adding the papers README to organize upcoming content."}}
{"type":"item.completed","item":{"id":"item_90","type":"file_change","changes":[{"path":"/data/hypogenicai/workspaces/llms-expose-science-codex/papers/README.md","kind":"add"}],"status":"completed"}}
{"type":"item.completed","item":{"id":"item_91","type":"reasoning","text":"**Adding dataset README file**"}}
{"type":"item.completed","item":{"id":"item_92","type":"file_change","changes":[{"path":"/data/hypogenicai/workspaces/llms-expose-science-codex/datasets/README.md","kind":"add"}],"status":"completed"}}
{"type":"item.completed","item":{"id":"item_93","type":"file_change","changes":[{"path":"/data/hypogenicai/workspaces/llms-expose-science-codex/code/README.md","kind":"add"}],"status":"completed"}}
{"type":"item.completed","item":{"id":"item_94","type":"reasoning","text":"**Creating literature review draft**\n\nI'm preparing a concise literature review summarizing six key papers with an overview of the research area, common methodologies, and recommendations, to be added as a new markdown file."}}
{"type":"item.completed","item":{"id":"item_95","type":"file_change","changes":[{"path":"/data/hypogenicai/workspaces/llms-expose-science-codex/literature_review.md","kind":"add"}],"status":"completed"}}
{"type":"item.completed","item":{"id":"item_96","type":"reasoning","text":"**Adding resources summary file**\n\nI'm creating a resources.md file based on the template that summarizes counts with tables, using apply_patch to add it."}}
{"type":"item.completed","item":{"id":"item_97","type":"file_change","changes":[{"path":"/data/hypogenicai/workspaces/llms-expose-science-codex/resources.md","kind":"add"}],"status":"completed"}}
{"type":"item.completed","item":{"id":"item_98","type":"reasoning","text":"**Updating plan step statuses**"}}
{"type":"item.updated","item":{"id":"item_3","type":"todo_list","items":[{"text":"Collect keywords and shortlist target papers (search APIs/web) and download PDFs to papers/","completed":true},{"text":"Identify datasets, download/snapshot small samples, and document dataset instructions in datasets/","completed":true},{"text":"Find and clone relevant code repos to code/ with documentation","completed":true},{"text":"Write literature_review.md and resources.md; create papers/datasets/code READMEs; finalize completion marker","completed":false}]}}
{"type":"item.completed","item":{"id":"item_99","type":"reasoning","text":"**Finalizing completion marker command**\n\nThe existing template uses a quoted heredoc preventing timestamp evaluation, so I'll switch to an unquoted heredoc to ensure the timestamp is inserted dynamically when creating the completion marker file."}}
{"type":"item.started","item":{"id":"item_100","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_100","type":"command_execution","command":"/bin/bash -lc pwd","aggregated_output":"/data/hypogenicai/workspaces/llms-expose-science-codex\n","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_101","type":"reasoning","text":"**Creating completion marker**"}}
{"type":"item.started","item":{"id":"item_102","type":"command_execution","command":"/bin/bash -lc 'cat > .resource_finder_complete << EOF\nResource finding phase completed successfully.\nTimestamp: $(date -Iseconds)\nPapers downloaded: 6\nDatasets downloaded: 2\nRepositories cloned: 2\nEOF'","aggregated_output":"","exit_code":null,"status":"in_progress"}}
{"type":"item.completed","item":{"id":"item_102","type":"command_execution","command":"/bin/bash -lc 'cat > .resource_finder_complete << EOF\nResource finding phase completed successfully.\nTimestamp: $(date -Iseconds)\nPapers downloaded: 6\nDatasets downloaded: 2\nRepositories cloned: 2\nEOF'","aggregated_output":"","exit_code":0,"status":"completed"}}
{"type":"item.completed","item":{"id":"item_103","type":"reasoning","text":"**Marking plan as completed**"}}
{"type":"item.updated","item":{"id":"item_3","type":"todo_list","items":[{"text":"Collect keywords and shortlist target papers (search APIs/web) and download PDFs to papers/","completed":true},{"text":"Identify datasets, download/snapshot small samples, and document dataset instructions in datasets/","completed":true},{"text":"Find and clone relevant code repos to code/ with documentation","completed":true},{"text":"Write literature_review.md and resources.md; create papers/datasets/code READMEs; finalize completion marker","completed":true}]}}
{"type":"item.completed","item":{"id":"item_104","type":"agent_message","text":"Completed resource gathering for “Can LLMs Expose What Science Refuses to See?” with papers, datasets, code, and documentation in place.\n\n- Added six PDFs in `papers/` and cataloged them in `papers/README.md`; literature synthesis with methodologies, metrics, gaps, and recommendations is in `literature_review.md`.\n- Collected two OpenAlex bibliometric slices (climate adaptation; neglected tropical diseases) plus samples; documented download/usage in `datasets/README.md` with `.gitignore` guarding bulk data.\n- Cloned `code/jr-ai-scientist` and `code/awesome-scientific-llms`, summarized purposes in `code/README.md`.\n- Consolidated resource catalog and search notes in `resources.md`; completion marker at `.resource_finder_complete`.\n\nNext steps:\n1) Expand OpenAlex pulls via `cursor=*` and add SDG/affiliation filters for richer attention-vs-impact analysis.\n2) Stand up a RAG pipeline (e.g., SciTLDR + OpenAlex metadata) to auto-generate gap reports and evaluate with ROUGE/human checks."}}
{"type":"item.completed","item":{"id":"item_3","type":"todo_list","items":[{"text":"Collect keywords and shortlist target papers (search APIs/web) and download PDFs to papers/","completed":true},{"text":"Identify datasets, download/snapshot small samples, and document dataset instructions in datasets/","completed":true},{"text":"Find and clone relevant code repos to code/ with documentation","completed":true},{"text":"Write literature_review.md and resources.md; create papers/datasets/code READMEs; finalize completion marker","completed":true}]}}
{"type":"turn.completed","usage":{"input_tokens":1475784,"cached_input_tokens":1328768,"output_tokens":11231}}
